<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>IFT6266 project blog</title>
<!-- 2017-03-29 Wed 10:51 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Louis-Guillaume Gagnon" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="style1.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">IFT6266 project blog</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. <span class="timestamp-wrapper"><span class="timestamp">[2017-03-29 Wed 10:34]</span></span> Status report: convolutions!</a>
<ul>
<li><a href="#sec-1-1">1.1. Next steps</a></li>
</ul>
</li>
<li><a href="#sec-2">2. <span class="timestamp-wrapper"><span class="timestamp">[2017-03-21 Tue 10:28]</span></span> Status report</a>
<ul>
<li><a href="#sec-2-1">2.1. Dropout</a></li>
<li><a href="#sec-2-2">2.2. Early stopping</a></li>
<li><a href="#sec-2-3">2.3. Quantitative metric</a></li>
<li><a href="#sec-2-4">2.4. Output layer</a></li>
</ul>
</li>
<li><a href="#sec-3">3. <span class="timestamp-wrapper"><span class="timestamp">[2017-03-07 Tue 16:11]</span></span> Introduction</a>
<ul>
<li><a href="#sec-3-1">3.1. The Code</a></li>
<li><a href="#sec-3-2">3.2. Validation</a></li>
<li><a href="#sec-3-3">3.3. Baseline model</a></li>
<li><a href="#sec-3-4">3.4. Next step</a></li>
<li><a href="#sec-3-5">3.5. Note: Setting up the code on the hades cluster</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> <span class="timestamp-wrapper"><span class="timestamp">[2017-03-29 Wed 10:34]</span></span> Status report: convolutions!</h2>
<div class="outline-text-2" id="text-1">
<p>
The main change since my last report is that convolutional and pooling
layers are now implemented. This ended up being more straightforward
than I anticipated, using theano's conv2d and pool2d operations.
</p>

<p>
As usual, it is important to cross-check that my implementation is
working as intended. To do so, I've implemented the LeNet-inspired
network from deeplearning.net/tutorial/lenet.html and ran it on mnist.
</p>

<p>
<img src="./lenet_loss.png" alt="lenet_loss.png" /> <img src="./lenet_acc.png" alt="lenet_acc.png" />
</p>

<p>
Excellent! Then, I tried a long shot: blindly adapting the input and
output of that model to solve our image generation task. After 200
epoch, we have something like:
</p>


<div class="figure">
<p><img src="./test_image_longshot.jpg" alt="test_image_longshot.jpg" />
</p>
</div>

<p>
Which is underwhelming, to say the least. Oh well! As I've said, it
was a long shot.
</p>
</div>

<div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Next steps</h3>
<div class="outline-text-3" id="text-1-1">
<p>
I am currently in the process of implementing a recurrent layer such
that I can use the captions. I first idea I'd like to try is to train
some sort of auto-encoder on the captions to learn some useful
representation that could be fed to a downstream model. I'm curious to
see how much it would help.
</p>

<p>
After that layer is implemented, it would be worthwhile start
optimizing and trying more involved models such as GANs.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> <span class="timestamp-wrapper"><span class="timestamp">[2017-03-21 Tue 10:28]</span></span> Status report</h2>
<div class="outline-text-2" id="text-2">
<p>
A few things have changed since the last report.
</p>
</div>

<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Dropout</h3>
<div class="outline-text-3" id="text-2-1">
<p>
I implemented a dropout layer, which can be found at
<a href="https://github.com/gagnonlg/IFT6266-project/blob/master/network.py#L51">https://github.com/gagnonlg/IFT6266-project/blob/master/network.py#L51</a>. Following
the obvious implementation, it uses a theano "shared<sub>randomstream</sub>" to
sample a binary mask for each input vector. The randomstream can be
passed to the constructor such that we seed the generator only once
for all layers.
</p>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Early stopping</h3>
<div class="outline-text-3" id="text-2-2">
<p>
I tried to setup some early stopping mechanism, which monitors the
training loss. Perhaps unsurprisingly, I didn't have much sucess at
using this loss since it is not smooth at all. 
</p>

<p>
To gain insights into this, I looked the behaviour of the squared
distance between the generated and the target patch for one image in
isolation as a function of the training epoch. 
</p>


<div class="figure">
<p><img src="./se_vs_epoch.png" alt="se_vs_epoch.png" />
</p>
</div>

<p>
Note that this is an image from the validation set.  We can see that
it's not really a stable indication of the fit quality! It jumps
around <b>a lot</b>, especially early in the fit where essentially randomly
generated patches get better scores than later-on.
</p>

<p>
For now, I've simply turned-off this early stopping while I look for a better 
strategy. To mitigate the problem of the instability of the metric, maybe 
using a moving average to smooth things out and using a burn-in time of 
~ 100 epochs could work. 
</p>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Quantitative metric</h3>
<div class="outline-text-3" id="text-2-3">
<p>
I also looked into alternative metrics to quantify the quality of the
generated image, which as we all know is an interesting problem in
itself.  We can see an image as a collection of pixels, which
themselves can be viewed as instances of 256 different classes. In
this view, it is possible to measure a probability distribution over
pixel values for a given image. It is then possible to measure the
Kullback-Liebler divergence (KL) between this distribution and another
one.  KL measures (in a sense) how much 2 probability distribution
differs. We can choose to compare the distribution from the generated image
with the true image, or the distribution from the border.
</p>


<div class="figure">
<p><img src="./kl_vs_epoch.png" alt="kl_vs_epoch.png" />
</p>
</div>

<p>
This measure jumps around also, which suggest that smoothing might be
necessary, but it actually seems to be a good indication of the fit
quality. Surprisingly this works as well when comparing with the true
center patch or with the border, which suggest that unsupervised
learning might be possible. Of course, minimizing this divergence only
ensures that the pixels are sampled from the right distribution but it
cannot build the required structure. It could be used as an
unsupervised pre-training pass followd by a "traditionnal" supervised
fine-tuning pass.
</p>
</div>
</div>

<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="section-number-3">2.4</span> Output layer</h3>
<div class="outline-text-3" id="text-2-4">
<p>
It occured to me that it might be a bad idea to use sigmoid
activations at the last layer because they saturate and since I'm
training with MSE loss, the training could be made more difficult by
the vanishing gradient. I changed the last layer of my nominal model
and trained with the same other hyperparamters. After 1000 epochs, 
the training still has not converged:
</p>


<div class="figure">
<p><img src="./test_image_20298.png" alt="test_image_20298.png" />
</p>
</div>

<p>
This suggest that optimizing the hyperparameters is warranted. However,
I won't spend time doing that now before at least implementing a convolutional
layer (this is the next step!).
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> <span class="timestamp-wrapper"><span class="timestamp">[2017-03-07 Tue 16:11]</span></span> Introduction</h2>
<div class="outline-text-2" id="text-3">
<p>
So I think it is about time I write an introductory post for my
IFT6266 project blog. 
</p>

<p>
During the last few weeks I've been slowly but surely starting to
implement a baseline model for the inpainting task. To begin, I've
chosen a simple densely connected MLP which I think should perform
fairly well and is relatively easy to implement, even in straight
theano which I'm not really used to (mainly used Keras in the past).
</p>
</div>

<div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> The Code</h3>
<div class="outline-text-3" id="text-3-1">
<p>
The code can be found on github:
<a href="https://github.com/gagnonlg/IFT6266-project">https://github.com/gagnonlg/IFT6266-project</a>. Mainly of interest is the
"network.py" file which implements the basic blocks needed. I actually
take inspiration from the Keras abstraction of a model being a stack
of layers. Each layer follows a simple API consisting of five methods:
</p>

<dl class="org-dl">
<dt> Layer.expression(self, X) </dt><dd>Returns a theano symbolic expression
for this layer, given an input tensor. Defaults to X (the
identity).
</dd>
<dt> Layer.training<sub>expression</sub>(self, X) </dt><dd>In case the expression is
different at training time, as in the case of a batch
normalization layer for example. Defaults to Layer.expression.
</dd>
<dt> Layer.parameters(self, X) </dt><dd>Returns a list of trainable parameters
for which the gradient of the loss function must be
computed. Defaults to an empty list.
</dd>
<dt> Layer.reg<sub>loss</sub>(self, X) </dt><dd>Returns a regularization term to add to
the loss. This can be used to implement L2 weight regularization,
for example. Defaults to 0.
</dd>
<dt> Layer.updates(self, X) </dt><dd>Returns a list of updates to add to the
training function. Used in batch normalization, for example, to
update the running statistics on the mini-batches.
</dd>
</dl>

<p>
So Far, the following layers are defined:
</p>

<dl class="org-dl">
<dt> ScaleOffset </dt><dd>Scale and offset the input tensor.
</dd>
<dt> Clip </dt><dd>Clip the input tensor within defined bounds.
</dd>
<dt> LinearTransformation </dt><dd>X*W + b, a staple of MLP's. Also supports L2
regularization for W.
</dd>
<dt> ReLU </dt><dd>The rectified linear unit
</dd>
<dt> Sigmoid </dt><dd>The famous sigmoid activation
</dd>
<dt> BatchNorm </dt><dd>Batch normalization.
</dd>
</dl>

<p>
These layers are collected by a <b>Network</b> object, through its <b>add</b>
method. The <b>compile</b> methods actually creates the needed theano
functions. Right now mse loss is used and SGD with momentum is
implemented. The <b>train</b> method iteratively caches chunks of the
dataset in a shared variable from which minibatches are sampled for 
the training.
</p>
</div>
</div>

<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Validation</h3>
<div class="outline-text-3" id="text-3-2">
<p>
In my experience, it helps a lot to have a simple benchmark problem on
which good results are easy to attain fast, in order to validate that
everything is working when non-trivial changes are made. The
validation used here is the simple problem of fitting a noisy 
sinusoidal curve. See tests/test<sub>network</sub><sub>1</sub>.py. Using a 2-layer architecture
with 100 hidden units per hidden layer, batch normalization, relu activations
(but linear output) with some momentum and l2 regularization, the following
performance is obtained after only 10 epochs:
</p>


<div class="figure">
<p><img src="./sine_test.png" alt="sine_test.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> Baseline model</h3>
<div class="outline-text-3" id="text-3-3">
<p>
My baseline model for the inpainting problem is a simple 3-layer
densely connected MLP with 1000 units per layer, batch normalization,
ReLU hidden activations, momentum and l2 regularization. The output
consists of individual sigmoids for all pixels which are scaled
by 255. The model was allowed to train for 1000 epochs. The
performance obtained during the training can be visualised below for a
validation image:
</p>


<div class="figure">
<p><img src="./test_images.gif" alt="test_images.gif" />
</p>
</div>

<p>
The performance is actually better than I expected for such a
relatively simple model! The network is quickly able to get at least
the luminosity/color gradient right and later on resolves quite well
some of the features such as the arm. However, as the training
progresses, we can clearly see that the model tends to improve rather
slowly. 
</p>
</div>
</div>

<div id="outline-container-sec-3-4" class="outline-3">
<h3 id="sec-3-4"><span class="section-number-3">3.4</span> Next step</h3>
<div class="outline-text-3" id="text-3-4">
<p>
The obvious next step is to implement a convolutionnal layer which I
hope will help achieve better performance.
</p>
</div>
</div>

<div id="outline-container-sec-3-5" class="outline-3">
<h3 id="sec-3-5"><span class="section-number-3">3.5</span> Note: Setting up the code on the hades cluster</h3>
<div class="outline-text-3" id="text-3-5">
<p>
I tried working with the module system but it seemed like a real pain
to get a consistent environment setup for all the packages I needed so
I resorted to install anaconda, as suggested on a blog from last
year's IFT6266 course: <a href="https://ift6266.github.io/2016/02/19/install_theano/">https://ift6266.github.io/2016/02/19/install_theano/</a>.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Louis-Guillaume Gagnon</p>
<p class="date">Created: 2017-03-29 Wed 10:51</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.1.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
